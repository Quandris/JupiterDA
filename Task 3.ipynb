{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6200f39a",
   "metadata": {},
   "source": [
    "## Data Analyst - Take Home Test - Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6b10f",
   "metadata": {},
   "source": [
    "#### Q1. How would you generate investment ideas from a dataset containing information when executives buy and sell shares in the listed companies they work for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d061da",
   "metadata": {},
   "source": [
    "One hypothesis could be that we imitate their trades given that there is a wealth of literature out there supporting the notion of executives' generating abnormal returns trading their own company's shares.\n",
    "\n",
    "Generally speaking, this theory posits that insiders know what is going on in a company, so if they are buying their own stocks that can be interpreted as a bullish sign and a bearish sign if they are selling. It is worth highlighting that insiders may sell stocks because of a multitude of reasons, not necessarily because of their bearish view on its outlook.\n",
    "\n",
    "Based on this theory, we could build a regression model where the explained variable is the abnormal return relative to an appropriate benchmark over a given period following the publication of the executive trades. We could then employ statistical tests to determine if the abnormal return (i.e. the difference between the trading strategy and its benchmark) is significant in a statistical sense (e.g. by employing t-tests).\n",
    "\n",
    "It would probably make sense to use multiple periods covering only a few days/weeks and also a few months but probably less than the reporting period of the company, which is usually 6 months. \n",
    "\n",
    "As for potential explanatory variables, the question does not specify what the data contains in particular but in my opinion it is reasonable to assume that it has information on company names, director names, positions, transaction types, announcement dates, number of shares traded as well as prices. Many of these could be used as explanatory variables (i.e. features) for our model. However, in order to attain the highest possible explanatory power, it is worth considering adding further variables by obtaining market data and leveraging feature engineering.\n",
    "\n",
    "I listed out a few potential new features below:\n",
    "- Aggregated buy-sell (net purchase) indicators: *This could be calculated both on an absolute and relative basis compared to total insider and total market transactions for the given company on the given day.*\n",
    "\n",
    "\n",
    "- Cluster buying: *Indicating if multiple insiders are buying/selling at the same time.*\n",
    "\n",
    "\n",
    "- Market capitalisation: *Large companies are monitored by a lot of analysts and it may thus be more difficult to generate alpha with them.*\n",
    "\n",
    "\n",
    "- Trade size (number of shares) over total shares outstanding (free float): *A high value could be an indication of a strong conviction but at the same time a significant reduction in free-float could lead to governance issues.*\n",
    "\n",
    "\n",
    "- Company news from the previous few days: *Many ways to incorporate this e.g. through sentiment scores.*\n",
    "\n",
    "\n",
    "- Employee category: *Not all roles are created equal; some (e.g. C-suite) allows for better access to information.*\n",
    "\n",
    "\n",
    "- Reason for transaction: *Probably not available for most but could be a feature with strong explanatory power.*\n",
    "\n",
    "*(The categorical variables will need to be added one-hot-encoded.)*\n",
    "\n",
    "Before getting to build the model however, it is also important to weed out data points with probably little relevance/low information content for stock price prediction. They include regular small transactions that may be related to some pension plans or options awarded as part of the executives remuneration package. As a rule of thumb, small absolute and relative transactions should not be included either.\n",
    "\n",
    "\n",
    "`Practical challenges:`\n",
    "\n",
    "- Lack of data granularity / metadata. For example, a given purchase amount may represent a significant or an insignificant percentage of a person's personal wealth / regular income. Thus, we should be able to infer different levels of conviction, something we cannot do in absence of personal wealth / regular income information.\n",
    "\n",
    "\n",
    "- A given person may hold positions at multiple firms and/or is present in the data history for multiple firms. In absence of an identifier though, we can only map them together using their names, which is not ideal. A given person may have superior abilities to interpret corporate \n",
    "\n",
    "\n",
    "- If we want to use corporate positions either as a standalone feature or create categories from them, we bump into inconsistencies (e.g. a director could mean many different things at different firms) and missing data (no corporate position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3046abf",
   "metadata": {},
   "source": [
    "#### Q2. Armed with anonymised web traffic data, how could you use the information contained within the dataset to trade in a streaming platform of your choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e747a9f",
   "metadata": {},
   "source": [
    "The question is not exactly clear as to whether the anonymised web traffic data contains information only in regards to the streaming platform or it is just part of it, but I am going to assume the former. Also, given that I have never had any exposure to this kind of data, I am going to make some assumptions that may not hold in reality.\n",
    "\n",
    "In particular, I am going to assume that the data contains a kind of fingerprint id, which can be derived from a combination of multiple factors such as the user’s IP address and some technical details about their computer that allows us to differentiate between users to a large extent. \n",
    "\n",
    "I will further assume that the streaming platform in question is subscription only; that is, users cannot watch content for free in return for ads. The stock price of these kinds of streaming services is driven to a large extent by the rate with which they can increase their paying customer base. Thus, if we could leverage web traffic data to make predictions with reasonable confidence, then this could be used to inform our decision as to how to trade in the given stock. Note that management guidance and analysts’ estimates get priced in before publishing official numbers on customer growth and so our predictions can only be used as buy/sell signals if they markedly differ from market expectations. \n",
    "\n",
    "Given the subscription status is probably not available in the web traffic data, we need to introduce some proxy to determine the status of the customers. For example, users who have not interacted with the platform for more than three months are deemed to be lost customers (they no longer pay for the service).\n",
    "\n",
    "The amount of insight we can derive from the data depends to a large part on its granularity. We should be able to know, among others, the number of unique fingerprints (users) interacted with the platform, their geographical location, how much time they spent on the platform, which contents they consumed and for how long. \n",
    "\n",
    "This set of features would probably not be sufficient to make very good predictions about user growth (and churn rates). Thus, we will need to enrich our data via feature engineering and potentially by adding metadata. For example, the inclusion of geographical markets would make sense, as growth prospects are heavily influenced by the current level of market penetration and general market saturation levels.\n",
    "\n",
    "Importantly, these firms apply unique customer experiences that they achieve through many means including by clustering customers that have similar viewing patterns and then they recommend content that other customers in the same group liked but they have not watched yet. This sort of clustering would be a worthwhile addition to our feature set as well given it is reasonable to assume that they behave similarly in terms of continuing their membership, too. It is unlikely, however, that the web traffic data would have the necessary metadata to do a proper clustering; we could tell who watches the same content but we could not tell why (i.e. in what respects those contents are similar).\n",
    "\n",
    "My prediction methodology would consist of two steps. First, a classification model (e.g. logistic regression, SVM, decision tree variants etc.) fitted to historical data to determine what leads to the loss of customers. Here, the explained variable is customer churning proxy introduced earlier, whilst the explanatory variables are the features discussed above. This could help us predict how many of the previous users (i.e. fingerprints) would not be an active paying customer in the period predicted.\n",
    "\n",
    "The second part is a time series prediction model, for example SARIMA, where we attempt to predict the number of new subscriptions (i.e. fingerprints) within the reporting period. This model allows to account for seasonality, which is probably a factor affecting the time series (e.g. there tends to be a spike in number of customers around the festive season), and also it supports adding exogenous variables (i.e. readily available and engineered features) for a better prediction accuracy.\n",
    "\n",
    "`Practical challenges:`\n",
    "\n",
    "Differentiating between two people, who have vastly different content consumption patterns but use the same IP address and computer, is not possible simply by using fingerprints. A further challenge is posed by the fact that the web traffic data (probably) does not contain a lot of metadata around the contents consumed by the customers and so customer segmentation could be a real challenge as we could only determine what two users have both watched but not why. Further, user activity can change rapidly and patterns identified earlier may not last long, and so the greater the lag in data acquisition, the more likely that the conclusions drawn are no longer (that) relevant. \n",
    "Finally, subscription status is (probably) not included in web traffic, hence we need to resort to using proxies, which may or may not model subscription status accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5921cb",
   "metadata": {},
   "source": [
    "#### Q3. Using email-receipt data, how would you decide which company to buy in a fiercely competitive subindustry (food delivery, for example)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1b289",
   "metadata": {},
   "source": [
    "I’m going to use the food delivery industry as an example. Again, I do not have personal experience dealing with this sort of alternative data, so my assumptions may not hold true in real life.\n",
    "\n",
    "Sales surprises tend to result in changes in stock prices and so if we could predict future sales with reasonable accuracy and if this prediction was better than the management guidance as well as what the consensus view of analysts suggested, then it could be interpreted as a buy signal for that particular firm.\n",
    "Thus, I am going to briefly discuss how such a (regression) model could be constructed.\n",
    "\n",
    "Email receipts generated automatically at the time of the transaction, and so we have a good idea about the date and time of the sale. Although not mentioned in the description, I assume the data is anonymised here, too, but it should still contain a customer identifier allowing us to identify unique customers to a certain degree (a person may use multiple email addresses).  The amount of insight we can derive from the data relies heavily on its granularity. I am going to assume that it contains information regarding payment methods, the location of the customer, the restaurant / food business whose product got delivered to the customer, the products themselves such as their category (e.g. main, dessert etc.), ingredients/toppings, diet-friendliness (e.g. vegan) etc. and obviously price paid along with the delivery fee. (As far as I know, delivery companies have income from two sources. One, is the delivery fee paid directly by the customer, and two, they take a certain percentage of the total bill, although this cut may change from vendor to vendor.)\n",
    "\n",
    "We can use all of the above discussed as separate features in our models to predict future sales. In addition to that, we could generate further ones via feature engineering. One example could be to add a client category feature (with values such as individual, family, corporate) derived from aggregated purchase amounts / price paid.\n",
    "\n",
    "Our dataset most likely does not contain all of the relevant transactions and it can be biassed in all sorts of ways. Thus, if available, these models should be built by fitting historical feature values against historical sales numbers reported by the companies. Once we have enriched our data with previous financial reports, we can build various regression models (e.g. linear models with regularisation - e.g. Lasso/Ridge, support vector regressions, regression trees, XGBoost  etc.) where the reported results are the explained variable, and the explanatory variables are the various features discussed earlier.\n",
    "\n",
    "Ideally, we will have data for these features in a given quarter that the companies have not reported on yet but guidance and analyst estimates are available. In that case, using this data and the trained model’s parameters we could forecast expected sales for the quarter. If that exceeds expectations by a reasonable degree, it is worth considering buying into that company.\n",
    "\n",
    "`Practical challenges:`\n",
    "\n",
    "It is unlikely that we would have access to all transactions of a given company, let alone the entire market. This raises the question as to whether the sample we have is a representative sample of the population? Especially, if it was obtained via self-reporting then that could introduce bias with respect to the type of customers and it may also not provide us with a long enough period / ample data for every day in a year. This is particularly problematic because of the seasonal nature of the food delivery business (e.g. there are hikes around/over the weekend and on public holidays). Further, as I mentioned earlier, another complicating factor is that the amount the delivery company receives as sales revenue is not calculated by simply taking a fixed percentage of the amount paid by the customer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
